{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIBLE_DATA = {\n",
    "    'NIV',\n",
    "    'NKJV'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://my-bible-study.appspot.com\n",
    "df = pd.read_csv(\n",
    "    'data/NIV_fixed.csv', \n",
    "    sep=',', \n",
    "    escapechar='\\\\', \n",
    "    names=['book', 'chapter', 'verse', 'text']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text.values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate cost of using OpenAI's Embedding model `text-embedding-ada-002`.\n",
    "\n",
    "Ada uses the `cl100k_base` encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "ENCODING = tiktoken.encoding_for_model(EMBEDDING_MODEL)\n",
    "ENCODING_NAME = \"cl100k_base\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str = ENCODING_NAME) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def num_tokens_from_row(row: typing.Dict):\n",
    "    return num_tokens_from_string(row['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the number of tokens for a verse\n",
    "print(df['text'][10], num_tokens_from_string(df['text'][10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIV has missing verses, see https://en.wikipedia.org/wiki/List_of_New_Testament_verses_not_included_in_modern_English_translations\n",
    "# clean first before tokenizing to avoid errors\n",
    "clean_df = df.dropna() \n",
    "len(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['tokens'] = clean_df.apply(num_tokens_from_row, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(clean_df.tokens) # ~29 tokens per verse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ada embedding model pricing: https://openai.com/api/pricing/\n",
    "ADA_PRICING_PER_TOKEN = 0.0004 # for every 1k token\n",
    "total_cost = sum(clean_df.tokens) / 1000 * ADA_PRICING_PER_TOKEN\n",
    "total_cost # $0.36 cents to generate embeddings for the entire Bible?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to save the clean df correctly with escaped double quotes\n",
    "# clean_df.to_csv('data/NIV_clean.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get embeddings from Ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source .env file from project directory\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "def get_single_embedding(text: str, model=EMBEDDING_MODEL) -> typing.List[float]:\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n",
    "\n",
    "def get_multi_embeddings(texts: typing.List[str], model=EMBEDDING_MODEL) -> typing.List[typing.List[float]]:\n",
    "   texts = [text.replace(\"\\n\", \" \") for text in texts]\n",
    "   return [data['embedding'] for data in openai.Embedding.create(input=texts, model=model)['data']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_multi_embeddings(list(df[:10].text.values))\n",
    "embeddings, len(embeddings.data), embeddings.data[0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.iloc[0]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test embeddings\n",
    "emb = get_single_embedding(clean_df.iloc[0]['text'])\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Row:\n",
    "    book: int\n",
    "    chapter: int\n",
    "    verse: int\n",
    "    text: str\n",
    "\n",
    "@dataclass\n",
    "class RowEmbeddings(Row):\n",
    "    oai_embeddings: typing.List[float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "\n",
    "def _get_row(fp: Path) -> Iterator[Row]:\n",
    "    with fp.open() as f:\n",
    "        for row in csv.DictReader(f):\n",
    "            yield Row(\n",
    "                int(row['book']),\n",
    "                int(row['chapter']),\n",
    "                int(row['verse']),\n",
    "                row['text']\n",
    "            )\n",
    "\n",
    "def generate_embeddings(fp: Path):\n",
    "    for row in _get_row(fp):\n",
    "        yield RowEmbeddings(\n",
    "            row.book,\n",
    "            row.chapter,\n",
    "            row.verse,\n",
    "            row.text,\n",
    "            get_single_embedding(row.text)\n",
    "        )\n",
    "\n",
    "def _read_row_batches(fp: Path, batch_size: int = 50) -> Iterator[typing.List[Row]]:\n",
    "    with fp.open() as f:\n",
    "        batches = []\n",
    "        for row in csv.DictReader(f):\n",
    "            batches.append(Row(\n",
    "                int(row['book']),\n",
    "                int(row['chapter']),\n",
    "                int(row['verse']),\n",
    "                row['text']\n",
    "            ))\n",
    "            if len(batches) == batch_size:\n",
    "                yield batches\n",
    "                batches = []\n",
    "        \n",
    "        if batches:\n",
    "            yield batches\n",
    "\n",
    "def generate_embeddings_batch(fp: Path, batch_size: int = 50) -> Iterator[typing.List[RowEmbeddings]]:\n",
    "    for batch in _read_row_batches(fp, batch_size):\n",
    "        try:\n",
    "            embs = get_multi_embeddings([row.text for row in batch])\n",
    "            yield [RowEmbeddings(\n",
    "                row.book,\n",
    "                row.chapter,\n",
    "                row.verse,\n",
    "                row.text,\n",
    "                embs[idx]\n",
    "            ) for idx, row in enumerate(batch)]\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(traceback.format_exc())\n",
    "            print(f\"Problematic batch: {batch}\")\n",
    "            raise e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_parquet(fp: Path, data: typing.Any):\n",
    "    df = pd.DataFrame(data, columns=['book', 'chapter', 'verse', 'text', 'oai_embeddings'])\n",
    "    df.to_parquet(fp, compression='gzip', engine=\"fastparquet\", index=False, append=os.path.isfile(fp))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_parquet(csv_fp: Path, parquet_fp: Path):\n",
    "    for batch in generate_embeddings_batch(csv_fp):\n",
    "        append_to_parquet(\n",
    "            parquet_fp, \n",
    "            batch,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_parquet(\n",
    "    csv_fp=Path(\"data/NIV_clean.csv\"),\n",
    "    parquet_fp=Path(\"data/NIV_clean.parquet\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.read_parquet('data/NIV_clean.parquet', engine='fastparquet')\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_tensors = torch.tensor(pdf['oai_embeddings'])\n",
    "embs_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ada embeddings are normalized already\n",
    "torch.functional.norm(embs_tensors[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_tensors[0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = get_single_embedding('trinity')\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = torch.matmul(embs_tensors, torch.tensor(query))\n",
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_results = torch.topk(search_results, 10)\n",
    "top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_results = pdf.loc[top_results.indices][['book', 'chapter', 'verse', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tuple(np_row) for np_row in list(text_results.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ada_vector(query: str):\n",
    "    return torch.tensor(get_single_embedding(query))\n",
    "\n",
    "def get_search_results(query: str, embeddings: torch.Tensor, source: pd.DataFrame, k: int = 10, only_text = False):\n",
    "    query_vec = torch.tensor(get_single_embedding(query))\n",
    "\n",
    "    # cosine similarity: Ada embeddings are L2 normalized, so only require a dot product\n",
    "    # between the query and embedding vectors.\n",
    "    results = torch.topk(torch.matmul(embeddings, query_vec), k)\n",
    "\n",
    "    cols = ['text'] if only_text else ['book', 'chapter', 'verse', 'text']\n",
    "    results = source.loc[results.indices][cols]\n",
    "    return [tuple(np_row) for np_row in list(results.values)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.iloc[29258].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_search_results('what is the meaning of life according to Jesus?', embs_tensors, pdf, only_text=True)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_search_results('what is the Trinity?', embs_tensors, pdf, only_text=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating sentence embeddings using `sentence_tranformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "ST_EMBEDDING_MODEL = \"all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(ST_EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minilm_embs = model.encode(pdf.text)\n",
    "minilm_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test encode \n",
    "model.encode(list(pdf.text.values[:10])).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minilm_list = minilm_embs.tolist()\n",
    "len(minilm_list), len(minilm_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf['minilm_embeddings'] = minilm_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf['minilm_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minilm_embs = torch.tensor(pdf['minilm_embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(minilm_embs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "# need to load the parquet file containing the embeddings\n",
    "model = SentenceTransformer(ST_EMBEDDING_MODEL)\n",
    "\n",
    "class EmbeddingType(enum.Enum):\n",
    "    Ada = 'ada'\n",
    "    miniLM = 'minilm'\n",
    "\n",
    "def _get_query_vec(query: str, emb_type: EmbeddingType):\n",
    "    if emb_type == EmbeddingType.Ada:\n",
    "        return torch.tensor(get_single_embedding(query))\n",
    "    elif emb_type == EmbeddingType.miniLM:\n",
    "        return model.encode(query, convert_to_tensor=True)\n",
    "    else:\n",
    "        raise Exception(f\"No such embedding: {emb_type}\") \n",
    "\n",
    "def _get_embeddings(emb_type: EmbeddingType):\n",
    "    if emb_type == EmbeddingType.Ada:\n",
    "        return torch.tensor(pdf['oai_embeddings'])\n",
    "    elif emb_type == EmbeddingType.miniLM:\n",
    "        return torch.tensor(pdf['minilm_embeddings'])\n",
    "    else:\n",
    "        raise Exception(f\"No such embedding: {emb_type}\")\n",
    "\n",
    "def _get_search_results(query_vec, embeddings, source: pd.DataFrame, k: int = 10, only_text=False):\n",
    "    results = torch.topk(torch.matmul(embeddings, query_vec), k)\n",
    "    cols = ['text'] if only_text else ['book', 'chapter', 'verse', 'text']\n",
    "    results = source.loc[results.indices][cols]\n",
    "    return [tuple(np_row) for np_row in list(results.values)]\n",
    "\n",
    "def search(query: str, emb_type: EmbeddingType = EmbeddingType.Ada, only_text: bool =False):\n",
    "    query_vec = _get_query_vec(query, emb_type)\n",
    "    _embeddings = _get_embeddings(emb_type)\n",
    "\n",
    "    return _get_search_results(query_vec, _embeddings, source=pdf, only_text=only_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"what is the meaning of life?\"\n",
    "ada_result = search(query_str, EmbeddingType.Ada, only_text=True)\n",
    "ada_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minilm_result = search(query_str, EmbeddingType.miniLM, only_text=True)\n",
    "minilm_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bible_semsearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "311b9b3fcec81a2948a4f239a40b1506f66564e1a1440dc80411dfa12c6f1966"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
